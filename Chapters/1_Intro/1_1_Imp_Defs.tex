\section{Important Definitions and First Examples}

We will begin by defining the fundamental objects of study in this course. We will then provide some examples of these objects and discuss means of constructing them.

\subsection{Algebras}

We begin by recalling the notion of a bilinear map.

\begin{boxdefinition}[Bilinear Map]
    Let $V$ and $W$ be vector spaces. We say that a map $f: V \times W \to \C$ is \textbf{bilinear} if it is linear in each argument. That is, for all $v, v' \in V$, $w, w' \in W$ and $\lambda \in \C$, we have
    \begin{align*}
        \fof{v + v', w} = \fof{v, w} + \fof{v', w} \\
        \fof{v, w + w'} = \fof{v, w} + \fof{v, w'} \\
        \fof{\lambda v, w} = \lambda \fof{v, w} = \fof{v, \lambda w}
    \end{align*}
\end{boxdefinition}

We will be particularly interested in bilinear maps from a vector space to itself.

\begin{boxdefinition}[Algebra]
    An \textbf{algebra} is a vector space $A$ equipped with a bilinear map $\cdot: A \times A \to A$.
\end{boxdefinition}

\begin{boxconvention}
    Given any algebra $A$, we will often refer to the corresponding bilinear map $\cdot$ as the \textbf{multiplication} map of $A$, and denote $\cdot(x, y)$ as simply $x \cdot y$ or even $xy$ (where the definition of $\cdot$ is clear from the context) for any $x, y \in A$.
\end{boxconvention}

There are many different kinds of algebras. We will be particularly interested in Lie algebras and associative algebras.

\begin{boxdefinition}[Associative Algebras]
    We say that an algebra $A$ is \textbf{associative} if the multiplication map $\cdot$ is associative. That is, for all $x, y, z \in A$, we have
    \begin{align*}
        (x \cdot y) \cdot z = x \cdot (y \cdot z)
    \end{align*}
\end{boxdefinition}

We have all seen associative algebras before.

\begin{boxexample}[The Matrix Algebra]\label{Ch1:Eg:MatrixAlgebra}
    The set $\MnC$ of $n \times n$ matrices over $\C$ forms an associative algebra under matrix multiplication, known as the Matrix Algebra.
\end{boxexample}

We will come back to associative algebras soon enough. We will now define the main object of study in this module.

\begin{boxdefinition}[Lie Algebras]
    A \textbf{Lie algebra} is an algebra $L$ whose bilinear map $\brac{\cdot, \cdot}: L \times L \to L$ satisfies the following properties:
    \begin{enumerate}
        \item For all $x \in L$, we have $[x, x] = 0$.
        \item For all $x, y, z \in L$, we have
        \begin{align}
            \brac{x, \brac{y, z}} + \brac{y, \brac{z, x}} + \brac{z, \brac{x, y}} = 0
            \label{SP:eq:JacobiIdentity}
        \end{align}
    \end{enumerate}
    Such a bilinear map $\liebrac$ is known as a \textbf{Lie Bracket}, and~\eqref{SP:eq:JacobiIdentity} is known as the \textbf{Jacobi Identity}.
\end{boxdefinition}

\begin{remark}
    We immediately notice that the first condition (over not just $\C$ but any field) implies the fact that
    \begin{align}
        \brac{x, y} = - \brac{y, x}
        \label{SP:Eq:LieBracAntisymm}
    \end{align}
    One simply needs to apply bilinearity and the first condition to evaluate $\brac{x + y, x + y}$. This argument reverses nicely as well, but only over fields of characteristic $\neq 2$.
\end{remark} 

One may recall that the $\liebrac$ notation is often used in group theory to denote the \textbf{commutator} of two elements. The reason why the same notation is used for the Lie bracket is the following.

\begin{boxlemma}\label{Ch1:Lemma:CommBracket}
    Let $A$ be an associative algebra. Then, the commutator map $\brac{x, y} = xy - yx$ is a Lie bracket on $A$.
\end{boxlemma}
\begin{proof}  % Generated by Copilot
    Clearly, $\brac{x, x} = xx - xx = 0$ for all $x \in A$. We now show that $\liebrac$ satisfies~\eqref{SP:eq:JacobiIdentity}: for all $x, y, z \in A$, we have
    \begin{align*}
        \brac{x, \brac{y, z}} + \brac{y, \brac{z, x}} + \brac{z, \brac{x, y}} &= \brac{x, yz - zy} + \brac{y, zx - xz} + \brac{z, xy - yx} \\
        &= % x(yz - zy) - (yz - zy)x + y(zx - xz) - (zx - xz)y + z(xy - yx) - (xy - yx)z \\
        % &= xyz - xzy - yzx + yxz + yzx - yxz - zxy + zyx + zxy - zyx - xyz + yxz \\
        6xyz - 6xyz = 0
    \end{align*}
    where we skip over some of the intermediate computations because they are tedious and uninteresting.
\end{proof}

\Cref{Ch1:Lemma:CommBracket} gives us a large class of examples of Lie algebras. One of the most important of these is the following.

\begin{boxexample}[General Linear Lie Algebra]\label{Ch1:Eg:gl}
    For all $n \in \N$, the set of all $n \times n$ matrices forms a Lie algebra under the commutator bracket: this follows immediately from applying \Cref{Ch1:Lemma:CommBracket} to \Cref{Ch1:Eg:MatrixAlgebra}. We call this the \textbf{General Linear Lie Algebra}, denoted $\gl{n}$.
\end{boxexample}

\begin{boxconvention}
    We will denote by $\MnC$ the set of all $n \times n$ matrices, viewed (interchangeably) as a \textit{set}, a \textit{vector space} or an \textit{associative algebra}. When viewing it as a \textit{Lie algebra under the commutator bracket}, we will adopt the notation $\gl{n, \C}$, where $\C$ can be replaced by any field. We will usually abbreviate this to $\gl{n}$, because we will primarily work over $\C$.
\end{boxconvention}

Lastly, we will define the notion of an abelian Lie algebra.

\begin{boxdefinition}[Abelian Lie Algebra]
    A Lie algebra $A$ is said to be \textbf{abelian} if for all $x \in A$, we have $\brac{x, x} = 0$.
\end{boxdefinition}

The reason for this terminology is that if $A$ is an associative algebra whose multiplication map is commutative, then its commutator bracket is identically zero, making the corresponding Lie algebra abelian.

\begin{boxexample}\label{Ch1:Eg:gl1}
    Clearly, $\gl{1}$ is abelian: for all $x, y \in \gl{1} = \C$, we have $xy - yx = 0$.
\end{boxexample}

We will now define subalgebras and homomorphisms of algebras, which will allow us to construct more examples of algebras (Lie and otherwise).

\subsection{Subalgebras and Homomorphisms}

As with objects in any category, we have subobjects and morphisms. We will define these over general algebras and apply them to get more examples of Lie algebras.

\begin{boxdefinition}[Subalgebras]
    Let $A$ be an algebra. A \textbf{subalgebra} of $A$ is a subspace $B \subseteq A$ such that $B$ is closed under the multiplication map of $A$. That is, for all $x, y \in B$, we have $x \cdot y \in B$.
\end{boxdefinition}

\begin{boxconvention}
    Given an algebra $A$ and a subset $B \subseteq A$, we will denote the statement that $B$ is a subalgebra of $A$ by $B \leq A$.
\end{boxconvention}

\begin{boxdefinition}[Homomorphisms]
    Let $A$ and $B$ be algebras. A \textbf{homomorphism} $\phi: A \to B$ is a \underline{linear} map that respects the multiplication maps of $A$ and $B$. That is, for all $x, y \in A$, we have
    \begin{align*}
        \phiof{x \cdot y} = \phiof{x} \cdot \phiof{y}
    \end{align*}
\end{boxdefinition}

\begin{boxconvention}
    We will define Lie subalgebras to be subalgebras with respect to the algebra structure given by the Lie bracket, and we will define Lie algebra homomorphisms to be homomorphisms that respect the Lie bracket (ie, that are algebra homomorphisms with respect to the algebra structure given by the Lie bracket).
\end{boxconvention}

We have the following unsurprising result.

\begin{boxlemma}\label{Ch1:Lemma:im_ker_subalg}
    Let $A$ and $B$ be algebras, and let $\phi: A \to B$ be a homomorphism. Then,
    \begin{enumerate}[label= \normalfont\arabic*., noitemsep]
        \item $\pim{\phi} \leq B$
        \item $\pker{\phi} \leq A$
    \end{enumerate}
\end{boxlemma}
\begin{proof}
    First, note that $\pim{\phi}$ and $\pker{\phi}$ are both linear subspaces. It therefore only remains to show that they are closed under the multiplication maps of $A$ and $B$.
    % AI-generated proof with minor human edits
    \begin{enumerate}
        \item Fix $x, y \in \pim{\phi}$. Then, there exist $a, b \in A$ such that $\phiof{a} = x$ and $\phiof{b} = y$. Since $\phi$ is a homomorphism, we have
        \begin{align*}
            x \cdot y = \phiof{a} \cdot \phiof{b} = \phiof{a \cdot b} \in \pim{\phi}
        \end{align*}
        so $\pim{\phi}$ is closed under the multiplication map of $B$.
        \item Let $x, y \in \pker{\phi}$. Then, we have
        \begin{align*}
            \phiof{x \cdot y} = \phiof{x} \cdot \phiof{y} = 0 \cdot 0 = 0
        \end{align*}
        where the last equality follows from the fact that $\cdot$ is bilinear. Therefore, $x \cdot y \in \pker{\phi}$, and $\pker{\phi}$ is closed under the multiplication map of $A$.
    \end{enumerate}
\end{proof}

This allows us to construct another matrix Lie algebra.

\begin{boxexample}[The Special Linear Lie Algebra]\label{Ch1:Eg:sl}
    For all $n \in \N$, consider the trace map $\operatorname{Tr} : \gl{n} \to \gl{1}$. This is a (Lie) algebra homomorphism: for all $A, B \in \gl{n}$,
    \begin{align*}
        \Tr{\brac{A, B}} = \Tr{AB - BA} = \Tr{AB} - \Tr{BA} = 0 = \brac{\Tr{A}, \Tr{B}}
    \end{align*}
    because the Lie algebra $\gl{1}$ is abelian (see \Cref{Ch1:Eg:gl1}). By \Cref{Ch1:Lemma:im_ker_subalg}, its kernel, the set of all $n \times n$ matrices of trace zero, is a Lie subalgebra of $\gl{n}$. We call this the \textbf{Special Linear Lie Algebra}, denoted $\sl{n}$. More explicitly,
    \begin{align*}
        \sl{n} := \setst{A \in \gl{n}}{\Tr{A} = 0}
    \end{align*}
    and the fact that $\sl{n} \leq \gl{n}$ also makes it a Lie algebra in its own right.
\end{boxexample}

\begin{remark}
    In \Cref{Ch1:Eg:sl}, we have indirectly shown that
    \begin{align*}
        \pim{\liebrac} = \brac{\gl{n}, \gl{n}} \subseteq \sl{n}
    \end{align*}
    because of the unique property of the trace that $\Tr{AB} = \Tr{BA}$ for any $A, B \in \gl{n}$.
\end{remark}

Often, when working with finite-dimensional algebras, we work with bases. As one might expect, if we can show the algebra homomorphism property for a basis, we can show it in general.

\begin{boxlemma}\label{Ch1:Lemma:Basis_Hom_Alg_Hom}
    Let $A$ and $B$ be algebras. Let $\set{e_1, \ldots, e_n}$ be a basis of $A$. If $\phi : A \to B$ is a linear map such that $\phiof{e_i \cdot e_j} = \phiof{e_i} \cdot \phiof{e_j}$ for all $1 \leq i, j \leq n$, then $\phi$ is an algebra homomorphism.
\end{boxlemma}
\begin{proof}
    The result follows from bilinearity. Assume that $\phi$ is a linear map that preserves multiplication between basis elements. Then, for all $x, y \in A$, if we write
    \begin{align*}
        x = \sum_{k=1}^{n} \lambda_k e_k \qquad \text{ and } \qquad y = \sum_{k=1}^{n} \mu_k e_k
    \end{align*}
    for some $\lambda_k, \mu_k \in \C$, we have
    \begin{align*}
        \phiof{x \cdot y}
        &= \phiof{\parenth{\sum_{k=1}^{n} \lambda_k e_k} \cdot \parenth{\sum_{k=1}^{n} \mu_k e_k}} \\
        &= \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_i \mu_j \phiof{e_i \cdot e_j} \\
        &= \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_i \mu_j \parenth{\phiof{e_i} \cdot \phiof{e_j}} \\
        &= \parenth{\sum_{i=1}^{n} \lambda_i \phiof{e_i}} \cdot \parenth{\sum_{j=1}^{n} \mu_j \phiof{e_j}} \\
        &= \phiof{x} \cdot \phiof{y}
    \end{align*}
    by bilinearity of $\cdot$ and linearity of $\phi$, giving us the desired result.
\end{proof}

The very natural relationship between associative and Lie algebra structures given by \Cref{Ch1:Lemma:CommBracket} gives us an elegant criterion for proving that a subspace is a subalgebra of a Lie algebra whose Lie bracket is the commutator of an associative bilinear map.

\begin{boxproposition}\label{Ch1:Prop:Subalg_Commbracket}
    Let $\parenth{A, \cdot_A}$ be an associative algebra and let $\parenth{B, \cdot_B}$ be a subalgebra of $A$. Denoting by $\parenth{A, \liebrac_A}$ the Lie algebra whose Lie bracket is the commutator of the multiplication map of $A$ and by $\parenth{B, \liebrac_B}$ the Lie algebra whose Lie bracket is the commutator of the multiplication map of $B$, we have $\parenth{B, \liebrac_B} \leq \parenth{A, \liebrac_A}$. In other words, the following diagram commutes:

    \begin{cd}
        \parenth{A, \cdot_A}
        \arrow[r] &[6em]
        \parenth{A, \liebrac_A} \\[2em]
        \parenth{B, \cdot_B}
        \arrow[r]
        \arrow[u, "\text{Associative Subalgebra}", hook] &[6em]
        \parenth{B, \liebrac_B}
        \arrow[u, "\text{Lie Subalgebra}"', hook', dashed]
        \label{Ch1:cd:Subalg_Commbracket}
    \end{cd}
\end{boxproposition}
\begin{proof}
    First, observe that $\liebrac_B = \liebrac_A\vert_B$ (ie, the Lie bracket obtained from $\cdot_B$ agrees with the one obtained from $\cdot_A$ on $B$): for all $T_1, T_2 \in B$,
    \begin{align*}
        \brac{T_1, T_2}_B = T_1 \cdot_B T_2 - T_2 \cdot_B T_1 = T_1 \cdot_A T_2 - T_2 \cdot_A T_1 = \brac{T_1, T_2}_A
    \end{align*}
    % The Lean coder in me wants to put coercion arrows everywhere... :,)
    Therefore, since $B$ is closed under $\liebrac_B$ (which, by definition, is a map from $B \times B$ to $B$), $B$ must be closed under $\liebrac_A$.
\end{proof}

This allows us to construct more examples still.

\begin{boxexample}[The Upper-Triangular Lie Algebra]\label{Ch1:Eg:UpperTriangularLieAlg}
    For $n \in \N$, we define the \textbf{Upper-Triangular Lie Algebra} to be the set of all $n \times n$ upper-triangular matrices (with respect to some predetermined basis), denoted $\t{n}$. Given that the product of upper-triangular matrices is upper-triangular, $\t{n}$ forms an associative subalgebra of $\MnC$, and therefore, a Lie subalgebra of $\gl{n}$.
\end{boxexample}

\subsection{Isomorphisms}

We would like to define an isomorphism of algebras to be an algebra homomorphism that is invertible, whose inverse is also an algebra homomorphism. It turns out, we can capture this information with a slightly simpler definition.

\begin{boxlemma}\label{Ch1:Lemma:Inv_of_Bij_Hom_Alg_Hom}
    Let $A$ and $B$ be algebras and $\phi : A \to B$ a bijection. If, in addition, $\phi$ is an algebra homomorphism, then so is $\phi\inv$.
\end{boxlemma}
\begin{proof}
    Assume $\phi$ is an algebra homomorphism. Since $\phi$ is linear, we know, from linear algebra, that $\phi\inv$ is linear as well. Therefore, all we need to show is that $\phi\inv$ respects the multiplication maps of $A$ and $B$. To that end, fix $y_1, y_2 \in B$. We need to show that
    \begin{align*}
        \phi\inv\of{y_1 \cdot y_2} = \phi\inv\of{y_1} \cdot \phi\inv\of{y_2}
    \end{align*}
    Since $\phi$ is a bijection, we know there are unique $x_1, x_2 \in A$ such that $\phiof{x_1} = y_1$ and $\phiof{x_2} = y_2$. Indeed, $x_1 = \phi\inv\of{y_1}$ and $x_2 = \phi\inv\of{y_2}$. Then,
    \begin{align*}
        \phi\inv\of{y_1 \cdot y_2} = \phi\inv\of{\phiof{x_1} \cdot \phiof{x_2}} = \phi\inv\of{\phiof{x_1 \cdot x_2}} = x_1 \cdot x_2 = \phi\inv\of{y_1} \cdot \phi\inv\of{y_2}
    \end{align*}
    as required, proving that $\phi\inv$ is a Lie algebra homomorphism too.
\end{proof}

It is therefore not necessary to require the inverse of an algebra homomorphism to be an algebra homomorphism in order for it to be an isomorphism, though this is a fact we get for free from \Cref{Ch1:Lemma:Inv_of_Bij_Hom_Alg_Hom}.

\begin{boxdefinition}[Isomorphism of Algebras]
    Let $A$ and $B$ be algebras. An \textbf{algebra isomorphism} $\phi : A \iso B$ is a bijective algebra homomorphism.
\end{boxdefinition}

Over the course of this module, we will see any number of pairs of isomorphic Lie algebras. We do not offer any examples here because there will be more than enough as we go along. We will instead move onto more important---and less trivial---concepts.

\subsection{Ideals}

Throughout this subsection, we will denote by $L$ an arbitrary Lie algebra.

\begin{boxdefinition}[Ideal]\label{Ch1:Def:Ideal}
    We say that $I \subseteq L$ is an \textbf{ideal} of $L$, denoted $I \nsg L$, if $I$ is a linear subspace of $L$ and $\brac{x, y} \in I$ for all $x \in L$ and $y \in I$.
\end{boxdefinition}

\begin{boxconvention}
    We will use the notation $\brac{L, I}$ to denote the subspace of $L$ spanned by all elements of the form $\brac{\ell, i}$ for $\ell \in L$ and $i \in I$.
\end{boxconvention}

\begin{remark}
    We could equivalently require that $\brac{I, L}\leq L$ in the definition of an ideal instead of requiring that $\brac{x, y} \in I$ for all $x \in L$ and $y \in I$. Similarly, we can observe that it doesn't matter whether we require $\brac{x, y} \in I$ or $\brac{y, x} \in I$ because of~\eqref{SP:Eq:LieBracAntisymm} and bilinearity.
\end{remark}

\begin{boxexample}[Trivial Ideals]
    Given any Lie algebra $L$, both $\set{0}$ and $L$ are ideals of $L$.
\end{boxexample}

In certain respects, despite their name, ideals of Lie algebras are more like normal subgroups of a group than they are like ideals of a ring.

\begin{boxlemma}\label{Ch1:Lemma:IdealSubalg}
    Any ideal $I \nsg L$ is also a subalgebra of $L$.
\end{boxlemma}
\begin{proof}
    This is clear from \Cref{Ch1:Def:Ideal}.
\end{proof}

\begin{boxlemma}
    For any Lie algebra $K$ and homomorphism $\phi : L \to K$, we have $\pker{\phi} \nsg L$.
\end{boxlemma}
\begin{proof}
    From \Cref{Ch1:Lemma:im_ker_subalg}, we know that $\pker{\phi}$ is a linear subspace of $L$. We now need to show that $\brac{x, y} \in \pker{\phi}$ for all $x \in L$ and $y \in \pker{\phi}$. To that end, fix $x \in \pker{\phi}$ and $y \in L$. Then,
    \begin{align*}
        \phiof{\brac{x, y}} = \brac{\phiof{x}, \phiof{y}} = \brac{0, \phiof{y}} = 0
    \end{align*}
    proving that $\brac{x, y} \in \pker{\phi}$ as required.
\end{proof}

We come back to the theme of the Lie bracket being some sort of `commutator' when we define the notion of the centre of a Lie algebra: the terminology and notation match those from group theory, where the centre consists of elements that commute with every other element of the group (making its commutator with every other element the identity).

\begin{boxdefinition}[The Centre of a Lie Algebra]
    We define the \textbf{centre} of $L$ to be
    \begin{align*}
        \Zof{L} := \setst{x \in L}{\forall y \in L, \ \brac{x, y} = 0}
    \end{align*}
\end{boxdefinition}

Just as the centre of a group is a normal subgroup, so too is the centre of a Lie algebra an ideal.

\begin{boxlemma}
    $\Zof{L}$ is an ideal of $L$.
\end{boxlemma}
\begin{proof}
    The fact that $\Zof{L}$ is a subspace of $L$ follows from the fact that $\liebrac$ is bilinear. Now, fix $x \in \Zof{L}$ and $y \in L$. Clearly, $\brac{x, y} = 0$, and it is easily seen that $0 \in \Zof{L}$.
\end{proof}

\begin{boxexample}
    For all $n \in \N$,
    \begin{align*}
        \Zof{\gl{n}} = \setst{A \in \gl{n}}{\exists \lambda \in \C \st A = \lambda I}
    \end{align*}
    \begin{proof}
        Let $S := \setst{A \in \gl{n}}{\exists \lambda \in \C \st A = \lambda I}$. It is clear that $S \subseteq \Zof{\gl{n}}$. Now, fix $A \in \Zof{\gl{n}}$. Then, for all $B \in \gl{n}$, we have that $\brac{A, B} = AB - BA = 0$. In particular, this implies that $A$ commutes with all the elementary matrices $E_{ij}$, which are the matrices with a $1$ in the $ij$-th position and $0$ elsewhere. Therefore, $A$ must be a diagonal matrix. 
    \end{proof}
\end{boxexample}

It turns out that ideals are well-behaved under several operations.

\begin{boxproposition}[The Behaviour of Ideals]\label{Ch1:Prop:IdealBhv}
    Let $I, J \nsg L$. Then,
    \begin{enumerate}[label = \normalfont\arabic*., noitemsep]
        \item $I + J \nsg L$
        \item $I \cap J \nsg L$
        \item $\brac{I, J} := \Span{\setst{\brac{i, j}}{i \in I, j \in J}} \nsg L$
    \end{enumerate}
\end{boxproposition}
\begin{proof}
    \hfill
    \begin{enumerate}
        \item Fix $x \in I + J$. Then, there exist elements $i \in I$ and $j \in J$ such that $x = i + j$. Then, for all $y \in L$, we have
        \begin{align*}
            \brac{x, y} = \brac{i + j, y} = \brac{i, y} + \brac{j, y}
        \end{align*}
        Since $I \nsg L$, we have $\brac{i, y} \in I$, and since $J \nsg L$, we have $\brac{j, y} \in J$. Therefore, $\brac{x, y} = \brac{i, y} + \brac{j, y} \in I + J$.

        \item Fix $x \in I \cap J$ and $y \in L$. Since $x \in I$ and $I \nsg L$, we have $\brac{x, y} \in I$. Similarly, since $x \in J$ and $J \nsg L$, we have $\brac{x, y} \in J$. Therefore, $\brac{x, y} \in I \cap J$.
        
        \item By bilinearity, the Lie bracket with some $y \in L$ of any (finite) linear combinations of elements of the form $\brac{i, j}$ (for $i \in I$ and $j \in J$) is a linear combination of elements of the form $\brac{\brac{i, j}, y}$. Therefore, it suffices to show that for all $i \in I$, $j \in J$, and $y \in L$, $\brac{\brac{i, j}, y} \in \brac{I, J}$.
        
        To that end, fix $i \in I$, $j \in J$ and $y \in L$. Then, applying \eqref{SP:Eq:LieBracAntisymm} followed by the Jacobi Identity \eqref{SP:eq:JacobiIdentity}, we have
        \begin{align*}
            \brac{\brac{i, j}, y} = -\brac{y, \brac{i, j}} = \brac{i, \brac{j, y}} + \brac{j, \brac{y, i}}
        \end{align*}
        Since $j \in J$ and $J \nsg L$, we have $\brac{j, y} \in L$, meaning that $\brac{i, \brac{j, y}} \in \brac{I, J}$. Similarly, since $i \in I$ and $I \nsg L$, we have $\brac{i, y} \in L$, meaning that $\brac{j, \brac{i, y}} = -\brac{\brac{i, y}, j} \in \brac{I, J}$. Therefore, $\brac{\brac{i, j}, y} = \brac{i, \brac{j, y}} + \brac{j, \brac{y, i}} \in \brac{I, J}$.
    \end{enumerate}
\end{proof}

The abelian case is particularly simple.

\begin{boxproposition}[Ideals of an Abelian Lie Algebra]\label{Ch1:Prop:SubspaceIdealOfAbelian}
    Let $L$ be abelian. Then, every sub-vector space of $L$ is an ideal of $L$.
\end{boxproposition}
\begin{proof}
    Let $I$ be a sub-vector space of $L$. Then, for all $x \in L$ and $y \in I$, we have $\brac{x, y} = 0$. Since $I$ is a subspace, we must have $0 \in I$, proving that $I$ is an ideal of $L$.
\end{proof}

We end by defining a special kind of ideal, which will become rather important.

\begin{boxdefinition}[Derived Subalgebra]\label{Ch1:Def:DerivedSubalg}
    The \textbf{derived subalgeba} of $L$, denoted $L'$, is the ideal (and subalgebra) $\brac{L, L}$.
\end{boxdefinition}

Note that $L'$ is, indeed, an ideal, by the third property proven in \Cref{Ch1:Prop:IdealBhv}.

\begin{boxconvention}
    Though $L'$ is an ideal, we will often refer to it as either the \textbf{derived subalgebra} or the \textbf{commutator subalgebra} of $L$. Indeed, \Cref{Ch1:Lemma:IdealSubalg} tells us that this is a reasonable, if not the most completely descriptive, thing to do.
\end{boxconvention}

We will end by giving a nice technique of showing that two subspaces of $L$ are ideals.

\begin{boxproposition}
    Let $I$ and $J$ be subalgebras of $L$ be such that $I + J = L$. If $\brac{I, I} \subseteq I$, $\brac{J, J} \subseteq J$, and $\brac{I, J} = \set{0}$, then both $I$ and $J$ are ideals of $L$.
\end{boxproposition}
\begin{proof}
    The key is that $\liebrac$ is bilinear. In other words, we have
    \begin{align*}
        \brac{L, I} = \brac{I + J, I} = \brac{I, I} + \brac{J, I} \subseteq I + \set{0} = I
    \end{align*}
    Therefore, $I$ is an ideal of $L$. By an identical computation, we can show that $\brac{L, J} = J$ as well, allowing us to conclude that $J$, too, is an ideal of $L$.
\end{proof}

\subsection{Quotients}

We now define the notion of a quotient (Lie) algebra. For the remainder of this subsection, let $L$ be a Lie algebra and $I$ an arbitrary ideal of $L$. Given that we already have a notion of $\quotient{L}{I}$---recall that $I$ is a subspace of $L$, meaning we can take the quotient in a linear algebraic sense---it seems only natural to attempt to define a Lie bracket on this vector space. It turns out that the definition of an ideal allows us to do this in a very natural way.

\begin{boxproposition}\label{Ch1:Prop:QuotientAlgebraLieBracket}
    Consider the vector space $\quotient{L}{I}$. The map $\liebrac : \quotient{L}{I} \times \quotient{L}{I} \to \quotient{L}{I}$ given by
    \begin{align}
        \brac{x + I, y + I} := \brac{x, y} + I
        \label{Ch1:Eq:QuotientBracket}
    \end{align}
    for all $x, y \in L$ is a Lie bracket on $\quotient{L}{I}$.
\end{boxproposition}
\begin{proof}
    We begin by showing that the Lie bracket on $\quotient{L}{I}$ is well-defined. Fix $x, x', y, y' \in L$ with $x - x' = i \in I$ and $y - y' = j \in I$, so that $x + I = x' + I$ and $y + I = y' + I$. Then,
    \begin{align*}
        \brac{x, y} - \brac{x', y'}
        &= \brac{x' + i, y' + j} - \brac{x', y'} \\
        &= \cancel{\brac{x', y'}} + \brac{i, y'} + \brac{x', j} + \brac{i, j} - \cancel{\brac{x', y'}} \\
        &= \brac{i, y'} + \brac{x', j} + \brac{i, j} \in I
    \end{align*}
    because $I$ is an ideal, proving that $\brac{x, y} + I = \brac{x', y'} + I$, making the choice of representative irrelevant and the bracket on $\quotient{L}{I}$ well-defined.

    From the definition of $\liebrac$ on $\quotient{L}{I}$, it is clear that $\brac{x + I, x + I} = 0$ for all $x \in L$. Now, for all $x, y, z \in L$, notice that
    \begin{align*}
        \brac{x + I, \brac{y + I, z + I}} = \brac{x + I, \brac{y, z} + I} = \brac{x, \brac{y, z}} + I
    \end{align*}
    The Jacobi identity follows immediately.
\end{proof}

\begin{boxdefinition}[Quotient Algebra]
    The \textbf{quotient algebra} of $L$ with respect to $I$ is the vector space $\quotient{L}{I}$ equipped with the bracket defined in \eqref{Ch1:Eq:QuotientBracket}, which we showed to be a Lie bracket in \Cref{Ch1:Prop:QuotientAlgebraLieBracket} above.
\end{boxdefinition}

\begin{boxexample}[Quotienting by the Derived Subalgebra]
    The quotient of $L$ by $L'$ is always an abelian Lie algebra.
\end{boxexample}

% WHEN YOU MOD OUT BY THE CENTRE OF A NONABELIAN, NILPOTENT LIE ALGEBRA, THE IMAGE OF THE CENTRE IN THE QUOTIENT IS ZERO, BUT THE QUOTIENT MIGHT STILL HAVE A NONZERO CENTRE.

The centre is particularly well-behaved under taking quotients, a fact we will use when studying a class of Lie algebras called \textit{nilpotent} Lie algebras.

\begin{boxproposition}
    Let $\phi : L \surj \quotient{L}{\Zof{L}}$ be the quotient epimorphism. Then, $\phiof{\Zof{L}} = \Zof{\phiof{L}} = \Zof{\quotient{L}{\Zof{L}}}$.
\end{boxproposition}

Indeed, we can show that the map $x \mapsto x + I : L \to \quotient{L}{I}$ is a Lie algebra homomorphism. More generally, we have the following results. % Correct term?

\subsection{Isomorphism Theorems}

Our favourite isomorphism theorems do, indeed, hold in the category of Lie algebras. Throughout this subsection, let $L$ be a Lie algebra.

\begin{boxtheorem}[First Isomorphism Theorem]\label{SP:Thm:FirstIso}
    Let $K$ be a Lie algebra and $\phi : L \to K$ a Lie algebra homomorphism. Then,
    \begin{align}
        \quotient{L}{\pker{\phi}} \cong \pim{\phi}
        \label{SP:Eq:FirstIsomorphism}
    \end{align}
\end{boxtheorem}

\begin{boxtheorem}[Second Isomorphism Theorem]\label{SP:Thm:SecondIso}
    Let $I, J \nsg L$. Then,
    \begin{align}
        \quotient{I + J}{I} \cong \quotient{J}{I \cap J}
        \label{SP:Eq:SecondIsomorphism}
    \end{align}
\end{boxtheorem}

% Keep adding to this section as needed

We also have a correspondence between ideals of $L$ and ideals of $\quotient{L}{I}$.

\begin{boxtheorem}[The Correspondence Theorem]\label{SP:Thm:Correspondence}
    Let $I \nsg L$. Then, there is a one-to-one correspondence between the ideals of $L$ containing $I$ and the ideals of $\quotient{L}{I}$. Ie, there is a bijection
    \begin{align}
        \setst{J \nsg L}{J \supseteq I}
        \longleftrightarrow
        \set{J \nsg \quotient{L}{I}}
    \end{align}
    \label{SP:Eq:Correspondence}
\end{boxtheorem}

Note that each of the sets in \eqref{SP:Eq:Correspondence} is partially ordered by inclusion.

\subsection{Adjoints}

Throughout this subsection, $V$ will refer to a finite-dimensional vector space.

We begin with a general Lie algebra construction.

\begin{boxdefinition}[General Linear Lie Algebra over an Arbitrary Vector Space]\label{Ch1:Def:gl_V}
    We define the \textbf{General Linear Lie Algebra over $V$} to be the set of all linear maps from $V$ to $V$, viewed as a Lie algebra under the commutator bracket on the associative algebra structure given by composition of linear maps (cf. \Cref{Ch1:Lemma:CommBracket}). We denote it $\gl{V}$.
\end{boxdefinition}

That this is, indeed, a Lie algebra should come as no surprise. Given that this construction is well-defined over \textit{any} vector space, we can, in particular, apply it to Lie algebras.

For the remainder of this subsection, let $L$ denote an arbitrary Lie algebra. It turns out that we can define a rather nice map that relates $L$ with $\gl{L}$: the adjoint.

\begin{boxdefinition}[Adjoint Map]\label{Ch1:Def:AdjointMap}
    To every $x \in L$, we can associate the linear map
    \begin{align*}
        \pad{x} : L \to L : y \mapsto \brac{x, y}
    \end{align*}
    We call this map the \textbf{adjoint map} associated to $x$.
\end{boxdefinition}

The fact that the Lie bracket is bilinear tells us that for all $x \in L$, $\pad{x}$ is indeed a linear map. That is, $\pad{x} \in \gl{L}$. We can therefore view $\ad$ as a map from $L$ to $\gl{L}$ which sends any $x \in L$ to the linear map $\pad{x} \in \gl{L}$ defined in \Cref{Ch1:Def:AdjointMap}. Now, observe that both $L$ and $\gl{L}$ are Lie algebras. It turns out that the adjoint map is compatible this fact.

\begin{boxproposition}\label{Ch1:Prop:AdjointLieAlgHom}
    The adjoint map $\ad : L \to \gl{L}$ is a Lie algebra homomorphism.
\end{boxproposition}
\begin{proof}
    That $\ad$ is linear follows from the fact that $\liebrac$ is bilinear. Now, fix $x, y \in L$, and consider the map $\pad{\brac{x, y}} \in \gl{L}$. We need to show that
    \begin{align*}
        \pad{\brac{x, y}} = \pad{x}\pad{y} - \pad{y}\pad{x}
    \end{align*}
    because the Lie bracket on $\gl{L}$ is the commutator with respect to composition of linear maps. To that end, fix $z \in L$. Then,
    \begin{align*}
        \parenth{\pad{x}\pad{y} - \pad{y}\pad{x}}\!(z)
        &= \pad{x}\!\parenth{\pad{y}\!(z)} - \pad{y}\!\parenth{\pad{x}\!(z)} & \\
        &= \pad{x}\!\parenth{\brac{y, z}} - \pad{y}\!\parenth{\brac{x, z}} & \\
        &= \brac{x, \brac{y, z}} - \brac{y, \brac{x, z}} & \\
        &= \brac{x, \brac{y, z}} + \brac{y, \brac{z, x}} & \text{(by \eqref{SP:Eq:LieBracAntisymm})} \\
        &= - \brac{z, \brac{x, y}} & \text{(by the Jacobi Identity)} \\
        &= \brac{\brac{x, y}, z} \\
        &= \pad{\brac{x, y}}\!(z)
    \end{align*}
    showing that the Lie bracket in $\gl{L}$ of the adjoints is indeed the adjoint of the Lie bracket in $L$ and allowing us to conclude that $\ad$ is a homomorphism of Lie algebras.
\end{proof}

We also highlight the following fact.
\begin{boxlemma}
    $\pker{\ad} = \Zof{L}$
\end{boxlemma}
\begin{proof}
    This is immediate from unfolding definitions.
\end{proof}

Finally, we discuss a rather interesting fact about the image of the adjoint map.

\begin{boxconvention}
    Denote by $\pad{L}$ the set $\pim{\ad} = \setst{\pad{x}}{x \in L}$.
\end{boxconvention}

We then have the following rather straightforward fact.

\begin{boxlemma}
    The set $\pad{L}$ is a Lie subalgebra of $\gl{L}$.
\end{boxlemma}
\begin{proof}
    This is immediate from \Cref{Ch1:Lemma:im_ker_subalg}.
\end{proof}

\subsection{Derivations}

Throughout this subsection, let $A$ be an arbitrary algebra with multiplication $\cdot$.

\begin{boxdefinition}\label{Ch1:Def:Derivation}
    We say that a linear map $D : A \to A$ is a \textbf{derivation} if it satisfies the Leibniz rule, ie, if
    \begin{align}
        D(x \cdot y) = x \cdot D(y) + D(x) \cdot y
        \label{Ch1:Eq:LeibnizRule}
    \end{align}
    for all $x, y \in A$.
\end{boxdefinition}
\begin{boxconvention}
    We will denote the set of all derivations of an algebra $A$ by $\Der{A}$.
\end{boxconvention}

Most readers will have encountered derivations before. We give below a classic example (over $\R$, for the first time so far) that the reader is sure to recognise.

\begin{boxexample}
    The space $C^{\infty}(\R)$ of smooth $\R \to \R$ functions is an $\R$-algebra under pointwise addition and multiplication. The differentiation map $D : C^{\infty}(\R) \to C^{\infty}(\R) : f \mapsto f'$ is easily seen to be a derivation.
\end{boxexample}

Recall that since $A$ is a vector space, $\gl{A}$ is a Lie algebra with respect to the commutator bracket (cf. \Cref{Ch1:Def:gl_V}). It turns out there is a relationship between $\Der{A}$ and $\gl{A}$.

\begin{boxproposition}\label{Ch1:Prop:DerLieSubalg}
    $\Der{A}$ is a Lie subalgebra of $\gl{A}$.
\end{boxproposition}
\begin{proof}
    That $\Der{A}$ is a subspace of $\gl{A}$ is not too difficult to show: it is clear that the zero map satisfies \eqref{Ch1:Eq:LeibnizRule}, and it readily follows from the bilinearity of $\cdot$ that $\Der{A}$ is closed under addition and scalar multiplication.

    We now need to show that $\Der{A}$ is closed under the commutator bracket. Fix $D, E \in \Der{A}$. We need to show that $\brac{D, E} = DE - ED$ satisfies \eqref{Ch1:Eq:LeibnizRule}. Indeed, for all $x, y \in A$,
    \begin{align*}
        \parenth{DE - ED}\!(x \cdot y)
        &= D\!\parenth{E(x \cdot y)} - E\!\parenth{D(x \cdot y)} \\
        &= D\!\parenth{x \cdot E(y) + E(x) \cdot y} - E\!\parenth{x \cdot D(y) + D(x) \cdot y} 
        % &= x \cdot D\!\parenth{E(y)} + D(x) \cdot E(y) + E(x) \cdot D(y) + D\!\parenth{E(x)} \cdot y
    \end{align*}
    which can be simplified, if tediously, to the desired form.
\end{proof}

Differentiation of smooth functions is not the only example of a derivation. In fact, in the context of Lie algebras, we have the following rather interesting fact.

\begin{boxproposition}\label{Ch1:Prop:ad_mem_Der}
    For all $x \in L$, the adjoint map $\pad{x} : L \to L : y \mapsto \brac{x, y}$ associated with $x$ is a derivation.
\end{boxproposition}
\begin{proof}
    We already know that $\pad{x} \in \gl{L}$. It only remains to show that $\pad{x}$ satisfies \eqref{Ch1:Eq:LeibnizRule} with respect to $\liebrac$. To that end, fix $y, z \in L$. Then, we have that
    \begin{align*}
        \pad{x}\!\parenth{\brac{y, z}}
        &= \brac{x, \brac{y, z}} \\
        &= - \brac{y, \brac{z, x}} - \brac{z, \brac{x, y}} \\
        &= \brac{y, \brac{x, z}} + \brac{\brac{x, y}, z} \\
        &= \brac{y, \pad{x}\!(z)} + \brac{\pad{x}\!(y), z}
    \end{align*}
    as required.
\end{proof}

We therefore have the following chain of Lie algebra inclusions.

\begin{boxlemma}\label{Ch1:Lemma:adSubalgDer}
    $\pad{L} \leq \Der{L} \leq \gl{L}$.
\end{boxlemma}
\begin{proof}
    We only need to show that $\pad{L} \leq \Der{L}$, because we have already shown that $\Der{L} \leq \gl{L}$ in \Cref{Ch1:Prop:DerLieSubalg}. By \Cref{Ch1:Prop:ad_mem_Der}, we know $\pad{L} \subseteq \Der{L}$. We know that $\pad{L}$ is a linear subspace of $\Der{L}$ because it is a linear subspace of $\gl{L}$ that is contained in $\Der{L}$. For the same reason, it is also a Lie subalgebra of $\Der{L}$, because we know that the commutator of two adjoints is the adjoint of some Lie bracket in $L$ (cf. \Cref{Ch1:Prop:AdjointLieAlgHom}). In particular, it is contained in $\Der{L}$ because it is the adjoint of some element of $L$ (by \Cref{Ch1:Prop:ad_mem_Der}). Therefore, $\pad{L}$ is a Lie subalgebra of $\Der{L}$.
\end{proof}

\Cref{Ch1:Lemma:adSubalgDer} is hardly surprising, given the facts we have already proven. What is a lot less obvious, though, is that $\pad{L}$ is more than just a subalgebra of $\Der{L}$: it is an ideal. We begin by proving a cute fact.

\begin{boxlemma}\label{Ch1:Lemma:commbrac_der_ad_eq_ad_der}
    For all $\ell \in L$ and $D \in \Der{L}$,
    \begin{align*}
        \brac{D, \pad{\ell}} = \pad{D\of{\ell}}
    \end{align*}
\end{boxlemma}
\begin{proof}
    Fix $\ell \in L$ and $D \in \Der{L}$. For all $x \in L$, bearing in mind that the $\liebrac$ notation here is used for both the commutator bracket in $\gl{L}$ and the Lie bracket in $L$,
    \begin{align*}
        \brac{D, \pad{\ell}}\of{x}
        &= \parenth{D \circ \pad{\ell} - \pad{\ell} \circ D}\of{x} \\
        &= D\of{\pad{\ell}\of{x}} - \pad{\ell}\of{D\of{x}} \\
        % &= D\of{\brac{\ell, x}} - \pad{\ell}\of{\brac{x, D\of{y}} + \brac{D\of{x}, y}} \nonumber \\
        &= D\of{\brac{\ell, x}} - \brac{\ell, D\of{x}} \\
        &= \brac{\ell, D\of{x}} + \brac{D\of{\ell}, x} - \brac{\ell, D\of{x}} \\
        &= \brac{D\of{\ell}, x}
    \end{align*}
    as required.
\end{proof}

It follows quite readily that $\pad{L} \nsg \Der{L}$.

\begin{boxcorollary}
    $\pad{L}$ is an ideal of $\Der{L}$ in $\gl{L}$.
\end{boxcorollary}
\begin{proof}
    \Cref{Ch1:Lemma:commbrac_der_ad_eq_ad_der} clearly tells us the commutator of a derivation and an adjunction is an adjunction, which is exactly what it means for $\pad{L}$ to be an ideal of $\Der{L}$ in $\gl{L}$.
    % THE FOLLOWING (from an older version of the notes) IS UNNECESSARY! An adjoint composed with a derivation is a derivation for the same reason that a derivation composed with a derivation is a derivation, because an adjoint is a derivation (by the previous result). What's a lot less trivial, and what's required to prove this result, is the fact that an adjoint composed with a derivation is again an adjoint.
    \begin{comment}
    derivation. To that end, fix $x, y \in L$. We need to show that
    \begin{align}
        \brac{D, \pad{\ell}}\of{\brac{x, y}} = \brac{x, \brac{D, \pad{\ell}}\of{y}} + \brac{\brac{D, \pad{\ell}}\of{x}, y}
        \label{Ch1:Eq:ad_ideal_der}
    \end{align}
    Indeed, we have that
    \begin{align}
        \brac{D, \pad{\ell}}\of{\brac{x, y}}
        &= \parenth{D \circ \pad{\ell} - \pad{\ell} \circ D}\of{\brac{x, y}} \nonumber \\
        &= D\of{\pad{\ell}\of{\brac{x, y}}} - \pad{\ell}\of{D\of{\brac{x, y}}} \nonumber \\
        % &= D\of{\brac{\ell, \brac{x, y}}} - \pad{\ell}\of{\brac{x, D\of{y}} + \brac{D\of{x}, y}} \nonumber \\
        &= D\of{\brac{\ell, \brac{x, y}}} - \brac{\ell, D\of{\brac{x, y}}} \nonumber \\
        &= \brac{\ell, D\of{\brac{x, y}}} + \brac{D\of{\ell}, \brac{x, y}} - \brac{\ell, D\of{\brac{x, y}}} \nonumber \\
        &= \brac{D\of{\ell}, \brac{x, y}} \label{Ch1:Eq:ad_ideal_der_LHS}
    \end{align}
    Similarly, bearing in mind that $\brac{D, \pad{\ell}}$ is the commutator in $\gl{L}$, we have
    \begin{align}
        \brac{x, \brac{D, \pad{\ell}}\of{y}} + \brac{\brac{D, \pad{\ell}}\of{x}, y}
        =& \brac{x, D\of{\pad{\ell}\of{y}} - \pad{\ell}\of{D\of{y}}} \nonumber \\
        &+ \brac{D\of{\pad{\ell}\of{x}} - \pad{\ell}\of{D\of{x}}, y} \nonumber \\
        =& \brac{x, D\of{\brac{\ell, y}}} - \brac{x, \brac{\ell, D\of{y}}} \nonumber \\
        &+ \brac{D\of{\brac{\ell, x}}, y} - \brac{\brac{\ell, D\of{x}}, y} \nonumber \\
        =& \brac{x, \brac{\ell, D(y)}} + \brac{x, \brac{D\of{\ell}, y}} - \brac{x, \brac{\ell, D\of{y}}} \nonumber \\
        &+ \brac{\brac{\ell, D(x)}, y} + \brac{\brac{D\of{\ell}, x}, y} - \brac{\brac{\ell, D\of{x}}, y} \nonumber \\
        =& \brac{x, \brac{D\of{\ell}, y}} + \brac{\brac{D\of{\ell}, x}, y} \nonumber \\
        =& -\brac{x, \brac{y, D\of{\ell}}} - \brac{y, \brac{D\of{\ell}, x}} \label{Ch1:Eq:ad_ideal_der_RHS}
    \end{align}
    where the last equality follows from antisymmetry \eqref{SP:Eq:LieBracAntisymm}. The Jacobi Identity \eqref{SP:eq:JacobiIdentity} then allows us to put \eqref{Ch1:Eq:ad_ideal_der_LHS} and \eqref{Ch1:Eq:ad_ideal_der_RHS} together and conclude that \eqref{Ch1:Eq:ad_ideal_der} holds, as required. Therefore, the commutator of a derivation and an adjoint is indeed a derivation, proving that $\pad{L}$ is indeed an ideal of $\Der{L}$ with respect to the Lie algebra structure it inherits from $\gl{L}$.
    \end{comment}
\end{proof}

The containment of $\pad{L}$ in $\Der{L}$ can be proper, though this is not necessary.

\begin{boxexample}[Adjunctions and Derivations in Abelian Lie Algebras]
    Let $L$ be an abelian Lie algebra. Then, any linear map $T \in \gl{L}$ is a derivation: for all $x, y \in L$,
    \begin{align*}
        T\of{\brac{x, y}} = T\of{0} = 0 = 0 + 0 = \brac{T\of{x}, y} + \brac{x, T\of{y}}
    \end{align*}
    Furthermore, since $\pad{L} = \set{0}$ (because $L$ is abelian), if $T$ is any non-zero linear map---for example, $T = \id_L$---then $T$ is a derivation that is not an adjunction, making the inclusion of $\pad{L}$ into $\Der{L}$ strict.
\end{boxexample}

\subsection{Structure Constants}

Fix $n \in \N$, and let $L$ be an $n$-dimensional Lie algebra. Consider some $\C$-basis $\B = \set{e_1, \ldots, e_n}$ of $L$. Given the fundamentally linear algebraic nature of Lie algebras, it is natural to study what happens when we apply the Lie bracket to elements of $\B$.

\begin{boxdefinition}[Structure Constants]\label{Ch1:Def:StructureConstants}
    Fix $i, j \in \set{1, \ldots, n}$. We know that there exist unique constants $s_{ij1}, s_{ij2}, \ldots, s_{ijn}$ such that
    \begin{align*}
        \brac{e_i, e_j} = \sum_{k = 1}^{n} s_{ijk} e_k
    \end{align*}
    We call the scalars $\set{s_{ijk}}_{1 \leq i, j, k \leq n}$ the \textbf{structure constants} of $L$ with respect to $\B$.
\end{boxdefinition}

In other words, the structure constant $s_{ijk}$ of $L$ with respect to $\B$ is the $k$th coordinate (with respect to $\B$) of the Lie bracket of the $i$th and $j$th elements of $\B$.

As one might expect from their name, the structure constants of a Lie algebra define it uniquely up to isomorphism.

\begin{boxtheorem}[The Meaning of Structure Constants]\label{Ch1:Thm:StructureConstantsMeaning}
    Let $L_1$ and $L_2$ be $n$-dimensional Lie algebras over $\C$. Then,
    $L_1 \cong L_2$ if and only if there exist $\C$-bases $\B_1$ and $\B_2$ of $L_1$ and $L_2$ respectively such that the structure constants of $L_1$ with respect to $\B_1$ are equal to those of $L_2$ with respect to $\B_2$.
\end{boxtheorem}
\begin{proof}
    First, note that the fact that $L_1$ and $L_2$ have the same dimension does \underline{not}, in general, make them isomorphic as Lie algebras. They are certainly isomorphic as $\C$-vector spaces, but this is not enough. For example, as we shall see later on (\Cref{Ch1:Subsec:LieAlgDim2}), there are two non-isomorphic Lie algebras of dimension $2$ over $\C$. With this in mind, we proceed with the proof.
    \begin{description}
        \item[$\parenth{\implies}$] Assume that $L_1 \cong L_2$ via some isomorphism $\phi : L_1 \to L_2$. Let $\B_1 = \set{e_1, \ldots, e_n}$ be any basis of $L_1$. We know that the set
        \begin{align*}
            \B_2 = \setst{f_i = \phiof{e_i}}{1 \leq i \leq n}
        \end{align*}
        is a basis of $L_2$. What's more, for all $1 \leq i, j \leq n$, since $\phi$ is an isomorphism of Lie algebras,
        \begin{align*}
            \phiof{\brac{e_i, e_j}} = \brac{\phiof{e_i}, \phiof{e_j}} = \brac{f_i, f_j}
        \end{align*}
        Therefore, denoting by $\set{s_{ijk}}_{1 \leq i, j, k \leq n}$ the structure constants of $L_1$ with respect to $\B_1$,
        \begin{align*}
            \brac{f_i, f_j} = \phiof{\brac{e_i, e_j}} = \phiof{\sum_{k=1}^{n} s_{ijk} e_k} = \sum_{k=1}^{n} s_{ijk} \phiof{e_k} = \sum_{k=1}^{n} s_{ijk} f_k
        \end{align*}
        By uniqueness of structure constants (ie, of coordinates), it must be that the structure constants of $L_2$ with respect to $\B_2$ are also $\set{s_{ijk}}_{1 \leq i, j, k \leq n}$.

        \item[$\parenth{\impliedby}$] Assume that there exist bases $\B_1 = \set{e_1, \ldots, e_n}$ and $\B_2 = \set{f_1, \ldots, f_n}$ of $L_1$ and $L_2$ respectively such tha the structure constants of $L_1$ with respect to $B_1$ are equal to those of $L_2$ with respect to $\B_2$. Denote these by $\set{s_{ijk}}_{1 \leq i, j, k \leq n}$. Define the linear isomorphism $\phi : L_1 \iso L_2$ that maps $e_k$ to $f_k$ for all $1 \leq k \leq n$. We show that $\phi$ is, in fact, a Lie algebra isomorphism. 
        
        We only need to show that $\phi$ is a Lie algebra homomorphism, ie, that it preserves Lie brackets. Since $L_1$ and $L_2$ have the same structure constants with respect to $\B_1$ and $\B_2$ respectively, we can see that for all $1 \leq i, j \leq n$,
        \begin{align*}
            \phiof{\brac{e_i, e_j}} = \phiof{\sum_{k=1}^{n} s_{ijk} e_k} = \sum_{k=1}^{n} s_{ijk} \phiof{e_k} = \sum_{k=1}^{n} s_{ijk} f_k = \brac{f_i, f_j} = \brac{\phiof{e_i}, \phiof{e_j}}
        \end{align*}
        \Cref{Ch1:Lemma:Basis_Hom_Alg_Hom} then allows us to conclude that $\phi$ preserves the Lie bracket for \textit{all} elements, not just basis elements. This makes it a Lie algebra homomorphism in addition to being a linear isomorphism, making it a Lie algebra isomorphism as desired.
    \end{description}
    \vspace{-1em}
\end{proof}

In a sense, structure constants give us a way of describing Lie algebraic phenomena numerically in the same way that coordinates give us a way of describing linear algebraic phenomena numerically. Indeed, both notions are basis-dependent. In fact, from the very definition of a Lie algebra, we can glean some information as to the behaviour of structure constants.

For the remainder of this subsection, just as we did at the beginning of this subsection, fix a Lie algebra $L$, a basis $\B = \set{e_1, \ldots, e_n}$, and structure constants $\set{s_{ijk}}_{1 \leq i, j, k \leq n}$ of $L$ with respect to $\B$.

\begin{boxlemma}\label{Ch1:Lemma:StructureConstantsBhv}
    For all $1 \leq i, j, k \leq n$,
    \begin{enumerate}
        \item $s_{iik} = 0$.
        \item $s_{ijk} = -s_{jik}$.
    \end{enumerate}
\end{boxlemma}
\begin{proof}
    We prove the first point using the fact that the Lie bracket of any element with itself is $0$. The proof of the second point is nearly identical, except it uses antisymmetry \eqref{SP:Eq:LieBracAntisymm} instead.
    
    Fix $1 \leq i, k \leq n$. From the definition of the Lie bracket, we know that $\brac{e_i, e_i} = 0$. Therefore, since $s_{iik}$ is the $k$th coordinate of $\brac{e_i, e_i}$ with respect to $\B$, $s_{iik}$ must be $0$.

    The proof of the second point is analogous.
\end{proof}

We can also describe the Jacobi Identity using structure constants, but this is significantly more cumbersome. Therefore, we do not do it here.

\begin{boxexample}[Computing Structure Constants in $\R^3$]
    The Euclidean space $\R^3$ can be viewed as a Lie algebra under the cross product: it is straightforward to show that the axioms of a Lie algebra are, indeed, satisfied. We can therefore compute the structure constants $\set{s_{ijk}}_{1 \leq i, j, k \leq 3}$ of $\R^3$ under the cross product with respect to the standard basis $\set{e_1 = \parenth{1, 0, 0}, e_2 = \parenth{0, 1, 0}, e_3 = \parenth{0, 0, 1}}$. \\ % \hfill \newline

    There are $27$ structure constants in total, but we can use \Cref{Ch1:Lemma:StructureConstantsBhv} to simplify things. Immediately, we can see that $9$ of them are zero:
    \begin{align*}
        s_{111} = s_{112} = s_{113} = s_{221} = s_{222} = s_{223} = s_{331} = s_{332} = s_{333} = 0
    \end{align*}
    Furthremore, of the remaining $18$, $9$ are the negatives of the other $9$. Therefore, we only really need to compute $9$ structure constants to know the remaining $27$. Since each structure constant is a coordinate of a vector, we only need to compute $3$ vectors, namely, the cross products of the basis vectors with each other. \\

    Observe that
    \begin{align*}
        e_1 \times e_2 = e_3 \qquad e_2 \times e_3 = e_1 \qquad e_3 \times e_1 = e_2
    \end{align*}
    This tells us that
    \begin{align*}
        % \begin{matrix}
            s_{121} &= 0 && s_{122} = 0 && s_{123} = 1 \\
            s_{231} &= 1 && s_{232} = 0 && s_{233} = 0 \\
            s_{311} &= 0 && s_{312} = 1 && s_{313} = 0
        % \end{matrix}
    \end{align*}
    Combining this with \Cref{Ch1:Lemma:StructureConstantsBhv}, we can see that the structure constants of $\R^3$ with respect to the standard basis are
    \begin{align*}
        % \begin{matrix}
            s_{111} &= 0 && s_{112} = 0 && s_{113} = 0 \\
            s_{121} &= 0 && s_{122} = 0 && s_{123} = 1 \\
            s_{131} &= 0 && s_{132} = -1 && s_{133} = 0 \\
            s_{211} &= 0 && s_{212} = 0 && s_{213} = -1 \\
            s_{221} &= 0 && s_{222} = 0 && s_{223} = 0 \\
            s_{231} &= 1 && s_{232} = 0 && s_{233} = 0 \\
            s_{311} &= 0 && s_{312} = 1 && s_{313} = 0 \\
            s_{321} &= -1 && s_{322} = 0 && s_{323} = 0 \\
            s_{331} &= 0 && s_{332} = 0 && s_{333} = 0
        % \end{matrix}
    \end{align*}
\end{boxexample}

%% TODO: Describe how structure constants give isomorphisms
% Also talk about how they relate to matrices and how we can describe identities in them like Jacobi and skew-symmetry

\subsection{Direct Sums}

In this subsection, we briefly describe the theory of the direct sum of two Lie algebras. Let $L_1$ and $L_2$ be arbitrary Lie algebras. Just as we did in \Cref{Ch1:Prop:QuotientAlgebraLieBracket}, we will define a Lie bracket on the vector space $L_1 \+ L_2$, and define the Lie algebra direct sum of $L_1$ and $L_2$ to be this vector space equipped with this bracket.

\begin{boxproposition}\label{Ch1:Prop:DirectSumLieBracket}
    Define the map $\liebrac : \parenth{L_1 \+ L_2} \times \parenth{L_1 \+ L_2} \to \parenth{L_1 \+ L_2}$ given by
    \begin{align}
        \brac{x_1 \+ x_2, y_1 \+ y_2} := \brac{x_1, y_1} \+ \brac{x_2, y_2}
        \label{Ch1:Eq:DirectSumBracket}
    \end{align}
    for all $x_1, y_1 \in L_1$ and $x_2, y_2 \in L_2$. Then, $\liebrac$ is a Lie bracket on $L_1 \+ L_2$.
\end{boxproposition}
\begin{proof} % Courtesy Copilot
    First, note that the Lie bracket $\liebrac$ is bilinear because it is bilinear on each component. Now, fix $x_1, y_1 \in L_1$ and $x_2, y_2 \in L_2$. Then, we have
    \begin{align*}
        \brac{x_1 \+ x_2, x_1 \+ x_2} &= \brac{x_1, x_1} \+ \brac{x_2, x_2} = 0 \+ 0 = 0
    \end{align*}
    proving that $\liebrac$ satisfies the first property of a Lie bracket. Finally, for all $z_1 \in L_1$ and $z_2 \in L_2$, observe that
    \begin{align*}
        \brac{x_1 \+ x_2, \brac{y_1 \+ y_2, z_1 \+ z_2}}
        &= \brac{x_1 \+ x_2, \brac{y_1, z_1} \+ \brac{y_2, z_2}} \\
        &= \brac{x_1, \brac{y_1, z_1}} \+ \brac{x_2, \brac{y_2, z_2}}
    \end{align*}
    and similarly for the other terms in the Jacobi identity. Since the Jacobi identity holds in $L_1$ and $L_2$, it must hold in $L_1 \+ L_2$. Therefore, $\liebrac$ is a Lie bracket on $L_1 \+ L_2$.
\end{proof}

\begin{boxdefinition}[Direct Sum]\label{Ch1:Def:DirectSum}
    The \textbf{direct sum} of $L_1$ and $L_2$ is the vector space $L_1 \+ L_2$ equipped with the bracket defined in \eqref{Ch1:Eq:DirectSumBracket}, which we showed to be a Lie bracket in \Cref{Ch1:Prop:DirectSumLieBracket} above.
\end{boxdefinition}

We can repeat this definition successively to define the direct sum of any finite number of Lie algebras. We will not explore this idea in any more detail and will take it for granted.

After all of this important theory, we are finally ready to study concrete Lie algebras and their properties. Given that the objective of this module is to classify all semi-simple Lie algebras over $\C$, a natural place to begin is classifying \textit{all} Lie algebras of small dimension. We will do this in the next section.
